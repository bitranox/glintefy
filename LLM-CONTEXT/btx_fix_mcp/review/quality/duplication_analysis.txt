************* Module btx_fix_mcp.tools_venv
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[238:274]
==btx_fix_mcp.subservers.review.perf:[191:229]
            log_result(self.logger, status == "SUCCESS", f"Analysis complete: {len(all_issues)} issues found")

            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts,
                metrics=self._compile_metrics(python_files, results, all_issues),
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Documentation Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            return []
        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]
        return [str(self.repo_path / f) for f in python_files]

    def _check_docstring_coverage(self) -> dict[str, Any]:
        """Check docstring coverage using interrogate.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[688:719]
==btx_fix_mcp.subservers.review.perf:[507:538]
            )
        lines.append("")
        return lines

    def _format_perf_header_section(self, verdict, files_count: int) -> list[str]:
        """Format report header with mindset and verdict."""
        return [
            "# Performance Analysis Report",
            "",
            "## Reviewer Mindset",
            "",
            self.mindset.format_header(),
            "",
            self.mindset.format_approach(),
            "",
            "## Verdict",
            "",
            f"**{verdict.verdict_text}**",
            "",
            f"- Critical issues: {verdict.critical_count}",
            f"- Warnings: {verdict.warning_count}",
            f"- Files analyzed: {files_count}",
            "",
        ]

    def _format_perf_overview_section(self, metrics: dict) -> list[str]:
        """Format overview section."""
        return [
            "## Overview",
            "",
            f"**Files Analyzed**: {metrics['files_analyzed']}", (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[245:274]
==btx_fix_mcp.subservers.review.security:[299:332]
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Security Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"

        if not files_list.exists():
            return []

        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]

        # Convert to absolute paths
        return [str(self.repo_path / f) for f in python_files]

    def _filter_existing_files(self, files: list[str]) -> list[str]:
        """Filter to only existing files.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[237:257]
==btx_fix_mcp.subservers.review.security:[294:317]
            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts,
                metrics=metrics.model_dump(),
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Security Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[480:503]
==btx_fix_mcp.subservers.review.perf:[438:461]
            issue_types = list({issue.get("type", "unknown") for issue in issues_dicts})

            # Cleanup old chunked files
            cleanup_chunked_issues(
                output_dir=report_dir,
                issue_types=issue_types,
                prefix="issues",
            )

            # Write chunked issues
            written_files = write_chunked_issues(
                issues=issues_dicts,
                output_dir=report_dir,
                prefix="issues",
            )

            if written_files:
                artifacts["issues"] = written_files[0]

        return artifacts

    def _compile_metrics(self, files: list[str], results: dict[str, Any], all_issues: list[BaseIssue]) -> dict[str, Any]:
        """Compile metrics for result.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[245:263]
==btx_fix_mcp.subservers.review.quality.__init__:[242:257]
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Quality Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            ) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[198:216]
==btx_fix_mcp.subservers.review.report:[162:180]
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Performance Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[76:95]
==btx_fix_mcp.subservers.review.quality.__init__:[109:130]
        base_config = get_config(start_dir=str(repo_path or Path.cwd()))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd()
        self.mcp_mode = mcp_mode

        # Initialize logger
        if mcp_mode:
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            self.logger = setup_logger(name, log_file=None, level=20)

        # Load config (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[283:298]
==btx_fix_mcp.subservers.review.docs:[248:263]
        log_error_detailed(
            self.logger,
            e,
            context={"repo_path": str(self.repo_path)},
            include_traceback=True,
        )
        return SubServerResult(
            status="FAILED",
            summary=f"# Dependency Analysis Failed\n\n**Error**: {e}",
            artifacts={},
            errors=[str(e)],
        )

    def _detect_project_type(self) -> str | None:
        """Detect project type from dependency files.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[58:79]
==btx_fix_mcp.subservers.review.security:[79:103]
        base_config = get_config(start_dir=str(repo_path))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        return input_dir, output_dir

    def _init_logger(self, name: str, mcp_mode: bool):
        """Initialize logger based on mode."""
        if mcp_mode:
            return get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            return setup_logger(name, log_file=None, level=20)

    def _apply_threshold_overrides(
        self,
        severity_threshold: str,
        confidence_threshold: str,
        config: dict | None,
    ) -> tuple[str, str]:
        """Apply threshold parameter overrides.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[729:744]
==btx_fix_mcp.subservers.review.perf:[541:557]
            "",
        ]

    def _format_perf_approval_section(self, verdict) -> list[str]:
        """Format approval status section."""
        lines = ["## Approval Status", "", f"**{verdict.verdict_text}**"]
        if verdict.recommendations:
            lines.append("")
            for rec in verdict.recommendations:
                lines.append(f"- {rec}")
        return lines

    def _generate_summary(self, results: dict[str, Any], all_issues: list[BaseIssue], files: list[str]) -> str:
        """Generate markdown summary with mindset evaluation."""
        metrics = self._compile_metrics(files, results, all_issues)
 (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[263:274]
==btx_fix_mcp.subservers.review.quality.files:[38:54]
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            return []
        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]
        return [str(self.repo_path / f) for f in python_files]

    def _check_docstring_coverage(self) -> dict[str, Any]:
        """Check docstring coverage using interrogate.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[239:248]
==btx_fix_mcp.subservers.review.scope:[168:177]
                summary=summary,
                artifacts=artifacts,
                metrics=metrics.model_dump(),
            )

        except Exception as e:
            log_error_detailed(
                self.logger,
                e, (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[158:171]
==btx_fix_mcp.subservers.review.perf:[120:133]
        missing = []

        # Check for files to analyze
        files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_code.txt"
            if not files_list.exists():
                missing.append(f"No files list found in {self.input_dir}. Run scope sub-server first.")

        return len(missing) == 0, missing

    def _run_docstring_coverage_check(self, results: dict, all_issues: list) -> None:
        """Run docstring coverage analysis.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.summary:[220:231]
==btx_fix_mcp.subservers.review.security:[633:645]
    return lines


def _build_approval_section(verdict: AnalysisVerdict) -> list[str]:
    """Build approval status section."""
    lines = ["", "## Approval Status", "", f"**{verdict.verdict_text}**"]
    if verdict.recommendations:
        lines.append("")
        for rec in verdict.recommendations:
            lines.append(f"- {rec}")
    return lines (duplicate-code)

------------------------------------------------------------------
Your code has been rated at 9.96/10 (previous run: 9.97/10, -0.00)

