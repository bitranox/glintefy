************* Module btx_fix_mcp.tools_venv
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[173:221]
==btx_fix_mcp.subservers.review.perf:[173:222]
            log_step(self.logger, 5, "Saving results")
            artifacts = self._save_results(results, all_issues)

            # Step 6: Generate summary
            summary = self._generate_summary(results, all_issues, python_files)

            # Determine status
            critical_count = len([i for i in all_issues if i.get("severity") == "critical"])
            status = "SUCCESS" if critical_count == 0 else "PARTIAL"

            log_result(
                self.logger, status == "SUCCESS",
                f"Analysis complete: {len(all_issues)} issues found"
            )

            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts,
                metrics=self._compile_metrics(python_files, results, all_issues),
            )

        except Exception as e:
            log_error_detailed(
                self.logger, e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Documentation Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            return []
        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]
        return [str(self.repo_path / f) for f in python_files]

    def _check_docstring_coverage(self) -> dict[str, Any]:
        """Check docstring coverage using interrogate.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[193:221]
==btx_fix_mcp.subservers.review.quality.__init__:[531:559]
            )

        except Exception as e:
            log_error_detailed(
                self.logger, e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Quality Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            return []
        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]
        return [str(self.repo_path / f) for f in python_files]

    def _get_js_files(self) -> list[str]:
        """Get JavaScript/TypeScript files to analyze.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[193:222]
==btx_fix_mcp.subservers.review.security:[242:274]
            )

        except Exception as e:
            log_error_detailed(
                self.logger, e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Performance Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            return []
        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]
        return [str(self.repo_path / f) for f in python_files]

    def _detect_patterns(self, files: list[str]) -> list[dict[str, Any]]:
        """Detect performance anti-patterns in code."""
        import re (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[395:417]
==btx_fix_mcp.subservers.review.perf:[403:425]
        critical_issues = [i for i in all_issues if i.get("severity") == "critical"]
        warning_issues = [i for i in all_issues if i.get("severity") == "warning"]

        verdict = evaluate_results(
            self.mindset,
            critical_issues,
            warning_issues,
            max(len(files), 1),
        )

        lines = [
            "# Documentation Analysis Report", "",
            "## Reviewer Mindset", "",
            self.mindset.format_header(), "",
            self.mindset.format_approach(), "",
            "## Verdict", "",
            f"**{verdict.verdict_text}**", "",
            f"- Critical issues: {verdict.critical_count}",
            f"- Warnings: {verdict.warning_count}",
            f"- Files analyzed: {len(files)}", "",
            "## Overview", "",
            f"**Files Analyzed**: {metrics['files_analyzed']}", (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[448:464]
==btx_fix_mcp.subservers.review.quality.__init__:[976:992]
        verdict = evaluate_results(
            self.mindset,
            critical_issues,
            warning_issues,
            total_items,
        )

        lines = [
            "# Quality Analysis Report", "",
            "## Reviewer Mindset", "",
            self.mindset.format_header(), "",
            self.mindset.format_approach(), "",
            "## Verdict", "",
            f"**{verdict.verdict_text}**", "",
            f"- Critical issues: {verdict.critical_count} ({verdict.critical_ratio:.1f}%)",
            f"- Warnings: {verdict.warning_count} ({verdict.warning_ratio:.1f}%)", (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality:[210:237]
==btx_fix_mcp.subservers.review.security:[250:277]
            return SubServerResult(
                status="FAILED",
                summary=f"# Security Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"

        if not files_list.exists():
            return []

        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]

        # Convert to absolute paths
        return [str(self.repo_path / f) for f in python_files]

    def _run_bandit(self, files: list[str]) -> list[dict[str, Any]]:
        """Run Bandit security scanner on files."""
        results = []

        # Filter to existing files only (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[201:221]
==btx_fix_mcp.subservers.review.quality:[210:235]
            return SubServerResult(
                status="FAILED",
                summary=f"# Quality Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze."""
        # Try files_code.txt first (Python files from scope)
        files_list = self.input_dir / "files_code.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_to_review.txt"

        if not files_list.exists():
            return []

        all_files = files_list.read_text().strip().split("\n")
        python_files = [f for f in all_files if f.endswith(".py") and f]

        # Convert to absolute paths
        return [str(self.repo_path / f) for f in python_files]

    def _analyze_complexity(self, files: list[str]) -> list[dict[str, Any]]:
        """Analyze cyclomatic complexity using radon.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[233:250]
==btx_fix_mcp.subservers.review.docs:[193:210]
            )

        except Exception as e:
            log_error_detailed(
                self.logger, e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Dependency Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _detect_project_type(self) -> str | None:
        """Detect project type from dependency files.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[60:79]
==btx_fix_mcp.subservers.review.perf:[65:84]
        base_config = get_config(start_dir=str(repo_path or Path.cwd()))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd()
        self.mcp_mode = mcp_mode

        # Initialize logger
        if mcp_mode:
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            self.logger = setup_logger(name, log_file=None, level=20)

        # Load config (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[193:210]
==btx_fix_mcp.subservers.review.report:[111:128]
            )

        except Exception as e:
            log_error_detailed(
                self.logger, e,
                context={"repo_path": str(self.repo_path)},
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Performance Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _get_python_files(self) -> list[str]:
        """Get Python files to analyze.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality:[117:138]
==btx_fix_mcp.subservers.review.security:[145:166]
            except Exception as e:
                self.logger.warning(f"Failed to load config: {e}")
                return None

        return None

    def validate_inputs(self) -> tuple[bool, list[str]]:
        """Validate inputs for quality analysis."""
        missing = []

        # Check for files to analyze
        files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            # Also check for files_code.txt
            files_list = self.input_dir / "files_code.txt"
            if not files_list.exists():
                missing.append(
                    f"No files list found in {self.input_dir}. "
                    "Run scope sub-server first."
                )
 (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[108:129]
==btx_fix_mcp.subservers.review.security:[88:109]
        base_config = get_config(start_dir=str(repo_path or Path.cwd()))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd()
        self.mcp_mode = mcp_mode

        # Initialize logger based on mode
        if mcp_mode:
            # MCP mode: stderr only (MCP protocol uses stdout)
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            # Standalone mode: stdout only (no file logging)
            self.logger = setup_logger(name, log_file=None, level=20)

        # Load config using extracted config module (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.__init__conf__:[61:71]
==scripts._utils:[584:594]
    fields = [
        ("name", name),
        ("title", title),
        ("version", version),
        ("homepage", homepage),
        ("author", author),
        ("author_email", author_email),
        ("shell_command", shell_command),
    ]
    pad = max(len(label) for label, _ in fields) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[107:122]
==btx_fix_mcp.subservers.review.perf:[110:125]
        missing = []

        # Check for files to analyze
        files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_code.txt"
            if not files_list.exists():
                missing.append(
                    f"No files list found in {self.input_dir}. Run scope sub-server first."
                )

        return len(missing) == 0, missing

    def execute(self) -> SubServerResult:
        """Execute performance analysis.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[70:118]
==btx_fix_mcp.subservers.review.quality:[30:81]
class QualitySubServer(BaseSubServer):
    """Comprehensive code quality analyzer.

    Orchestrates multiple specialized analyzers to provide comprehensive
    quality analysis of Python and JavaScript/TypeScript codebases.
    """

    def __init__(
        self,
        name: str = "quality",
        input_dir: Path | None = None,
        output_dir: Path | None = None,
        repo_path: Path | None = None,
        complexity_threshold: int | None = None,
        maintainability_threshold: int | None = None,
        max_function_length: int | None = None,
        max_nesting_depth: int | None = None,
        cognitive_complexity_threshold: int | None = None,
        config_file: Path | None = None,
        mcp_mode: bool = False,
    ):
        """Initialize quality sub-server.

        Args:
            name: Sub-server name
            input_dir: Input directory (contains files_to_review.txt from scope)
            output_dir: Output directory for results
            repo_path: Repository path (default: current directory)
            complexity_threshold: Complexity threshold for warnings
            maintainability_threshold: MI threshold for warnings
            max_function_length: Max lines per function
            max_nesting_depth: Max nesting depth
            cognitive_complexity_threshold: Cognitive complexity threshold
            config_file: Path to config file
            mcp_mode: If True, log to stderr only (MCP protocol compatible).
                      If False, log to stdout only (standalone mode).
        """
        # Get output base from config for standalone use
        base_config = get_config(start_dir=str(repo_path or Path.cwd()))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd() (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[60:73]
==btx_fix_mcp.subservers.review.quality:[261:275]
                            })
            except subprocess.TimeoutExpired:
                self.logger.warning(f"Timeout analyzing {file_path}")
            except json.JSONDecodeError:
                self.logger.warning(f"Invalid JSON from radon for {file_path}")
            except Exception as e:
                self.logger.warning(f"Error analyzing {file_path}: {e}")

        return results

    def _analyze_maintainability(self, files: list[str]) -> list[dict[str, Any]]:
        """Analyze maintainability index using radon."""
        results = [] (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality:[144:155]
==btx_fix_mcp.subservers.review.security:[183:194]
        try:
            # Step 1: Get files to analyze
            log_step(self.logger, 1, "Loading files to analyze")
            python_files = self._get_python_files()

            if not python_files:
                log_result(self.logger, True, "No Python files to analyze")
                return SubServerResult(
                    status="SUCCESS",
                    summary="# Security Analysis\n\nNo Python files to analyze.",
                    artifacts={}, (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[506:517]
==btx_fix_mcp.subservers.review.docs:[435:446]
        lines.append("")

        # Approval status
        lines.extend(["## Approval Status", ""])
        lines.append(f"**{verdict.verdict_text}**")
        if verdict.recommendations:
            lines.append("")
            for rec in verdict.recommendations:
                lines.append(f"- {rec}")

        return "\n".join(lines) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[239:250]
==btx_fix_mcp.subservers.review.scope:[168:183]
                include_traceback=True,
            )
            return SubServerResult(
                status="FAILED",
                summary=f"# Dependency Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _detect_project_type(self) -> str | None:
        """Detect project type from dependency files.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[60:70]
==btx_fix_mcp.subservers.review.quality:[69:81]
        base_config = get_config(start_dir=str(repo_path or Path.cwd()))
        output_base = base_config.get("review", {}).get("output_dir", "LLM-CONTEXT/btx_fix_mcp/review")

        if input_dir is None:
            input_dir = Path.cwd() / output_base / "scope"
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd()

        # Initialize logger (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[183:192]
==btx_fix_mcp.subservers.review.quality.__init__:[521:530]
            log_result(
                self.logger, status == "SUCCESS",
                f"Analysis complete: {len(all_issues)} issues found"
            )

            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts, (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[398:407]
==btx_fix_mcp.subservers.review.security:[385:394]
        verdict = evaluate_results(
            self.mindset,
            critical_issues,
            warning_issues,
            max(len(files), 1),
        )

        lines = [
            "# Documentation Analysis Report", "", (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[60:72]
==btx_fix_mcp.subservers.review.quality:[295:311]
                        })
            except subprocess.TimeoutExpired:
                self.logger.warning(f"Timeout analyzing {file_path}")
            except json.JSONDecodeError:
                self.logger.warning(f"Invalid JSON from radon for {file_path}")
            except Exception as e:
                self.logger.warning(f"Error analyzing {file_path}: {e}")

        return results

    def _identify_issues(
        self,
        complexity_results: list[dict],
        mi_results: list[dict],
    ) -> list[dict[str, Any]]:
        """Identify quality issues based on thresholds.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality:[107:116]
==btx_fix_mcp.subservers.review.security:[135:144]
            except Exception as e:
                self.logger.warning(f"Failed to load config: {e}")
                return None

        default_config = self.repo_path / ".btx-review.yaml"
        if default_config.exists():
            try:
                with open(default_config) as f:
                    full_config = yaml.safe_load(f) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[34:46]
==btx_fix_mcp.subservers.review.quality.metrics:[31:43]
        }

    def _analyze_cyclomatic(self, files: list[str]) -> list[dict[str, Any]]:
        """Analyze cyclomatic complexity using radon."""
        results = []
        radon = str(get_tool_path("radon"))

        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[364:377]
==btx_fix_mcp.subservers.review.perf:[377:390]
        if all_issues:
            path = self.output_dir / "issues.json"
            path.write_text(json.dumps(all_issues, indent=2))
            artifacts["issues"] = path

        return artifacts

    def _compile_metrics(
        self, files: list[str], results: dict[str, Any], all_issues: list[dict]
    ) -> dict[str, Any]:
        """Compile metrics for result."""
        return {
            "files_analyzed": len(files), (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[417:425]
==btx_fix_mcp.subservers.review.quality:[151:161]
                return SubServerResult(
                    status="SUCCESS",
                    summary="# Quality Analysis\n\nNo Python files to analyze.",
                    artifacts={},
                    metrics={"files_analyzed": 0},
                )

            log_file_list(self.logger, python_files, "Python files", max_display=10)

            # Step 2: Run complexity analysis (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.__init__:[1077:1085]
==btx_fix_mcp.subservers.review.security:[475:483]
        lines.extend(["", "## Approval Status", ""])
        lines.append(f"**{verdict.verdict_text}**")
        if verdict.recommendations:
            lines.append("")
            for rec in verdict.recommendations:
                lines.append(f"- {rec}")

        return "\n".join(lines) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[38:46]
==btx_fix_mcp.subservers.review.quality.metrics:[83:91]
        results = []
        radon = str(get_tool_path("radon"))

        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[72:80]
==btx_fix_mcp.subservers.review.quality.metrics:[35:43]
        results = []
        radon = str(get_tool_path("radon"))

        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[89:99]
==btx_fix_mcp.subservers.review.docs:[69:79]
        self.repo_path = repo_path or Path.cwd()
        self.mcp_mode = mcp_mode

        # Initialize logger
        if mcp_mode:
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            self.logger = setup_logger(name, log_file=None, level=20)

        # Load config (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[225:232]
==btx_fix_mcp.subservers.review.docs:[185:192]
                f"Analysis complete: {len(all_issues)} issues found"
            )

            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts, (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[414:426]
==btx_fix_mcp.subservers.review.docs:[364:376]
        if all_issues:
            path = self.output_dir / "issues.json"
            path.write_text(json.dumps(all_issues, indent=2))
            artifacts["issues"] = path

        return artifacts

    def _compile_metrics(
        self, results: dict[str, Any], all_issues: list[dict]
    ) -> dict[str, Any]:
        """Compile metrics for result."""
        return { (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[241:250]
==btx_fix_mcp.subservers.review.quality:[210:220]
            return SubServerResult(
                status="FAILED",
                summary=f"# Dependency Analysis Failed\n\n**Error**: {e}",
                artifacts={},
                errors=[str(e)],
            )

    def _detect_project_type(self) -> str | None:
        """Detect project type from dependency files.""" (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[510:517]
==btx_fix_mcp.subservers.review.quality.__init__:[1078:1085]
        lines.append(f"**{verdict.verdict_text}**")
        if verdict.recommendations:
            lines.append("")
            for rec in verdict.recommendations:
                lines.append(f"- {rec}")

        return "\n".join(lines) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[107:115]
==btx_fix_mcp.subservers.review.quality:[125:134]
        missing = []

        # Check for files to analyze
        files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_code.txt"
            if not files_list.exists():
                missing.append( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[439:446]
==btx_fix_mcp.subservers.review.security:[476:483]
        lines.append(f"**{verdict.verdict_text}**")
        if verdict.recommendations:
            lines.append("")
            for rec in verdict.recommendations:
                lines.append(f"- {rec}")

        return "\n".join(lines) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[141:149]
==btx_fix_mcp.subservers.review.quality:[151:158]
                return SubServerResult(
                    status="SUCCESS",
                    summary="# Performance Analysis\n\nNo Python files to analyze.",
                    artifacts={},
                    metrics={"files_analyzed": 0},
                )

            # Step 2: Pattern-based detection (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[377:389]
==btx_fix_mcp.subservers.review.quality.__init__:[893:909]
        if all_issues:
            path = self.output_dir / "issues.json"
            path.write_text(json.dumps(all_issues, indent=2))
            artifacts["issues"] = path

        return artifacts

    def _compile_metrics(
        self,
        python_files: list[str],
        js_files: list[str],
        results: dict[str, Any],
        all_issues: list[dict],
    ) -> dict[str, Any]:
        """Compile metrics for the result."""
        return { (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.perf:[110:118]
==btx_fix_mcp.subservers.review.security:[153:161]
        missing = []

        # Check for files to analyze
        files_list = self.input_dir / "files_to_review.txt"
        if not files_list.exists():
            files_list = self.input_dir / "files_code.txt"
            if not files_list.exists():
                missing.append( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.architecture:[141:150]
==btx_fix_mcp.subservers.review.quality:[269:281]
        return results

    def _analyze_maintainability(self, files: list[str]) -> list[dict[str, Any]]:
        """Analyze maintainability index using radon."""
        results = []

        for file_path in files:
            if not Path(file_path).exists():
                continue

            try:
                # Run radon mi (maintainability index) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[48:54]
==btx_fix_mcp.subservers.review.quality:[248:255]
                )

                if result.returncode == 0 and result.stdout.strip():
                    data = json.loads(result.stdout)
                    for filepath, functions in data.items():
                        for func in functions:
                            results.append({ (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality:[189:196]
==btx_fix_mcp.subservers.review.security:[229:236]
            )

            return SubServerResult(
                status=status,
                summary=summary,
                artifacts=artifacts,
                metrics={ (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.architecture:[45:51]
==btx_fix_mcp.subservers.review.quality.complexity:[101:108]
        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                content = Path(file_path).read_text(encoding="utf-8", errors="ignore")
                tree = ast.parse(content) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.architecture:[97:103]
==btx_fix_mcp.subservers.review.quality.complexity:[150:157]
        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                content = Path(file_path).read_text(encoding="utf-8", errors="ignore")
                tree = ast.parse(content) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.deps:[90:99]
==btx_fix_mcp.subservers.review.scope:[77:93]
        self.mcp_mode = mcp_mode

        # Setup logging based on mode
        if mcp_mode:
            # MCP mode: stderr only (MCP protocol uses stdout)
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            # Standalone mode: stdout only (no file logging)
            self.logger = setup_logger(name, log_file=None, level=20)  # INFO

    def validate_inputs(self) -> tuple[bool, list[str]]:
        """Validate inputs for scope analysis.

        Returns:
            Tuple of (valid, missing_items)
        """ (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.docs:[65:73]
==btx_fix_mcp.subservers.review.report:[48:54]
        if output_dir is None:
            output_dir = Path.cwd() / output_base / name

        super().__init__(name=name, input_dir=input_dir, output_dir=output_dir)
        self.repo_path = repo_path or Path.cwd()
        self.mcp_mode = mcp_mode

        # Initialize logger (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.architecture:[145:150]
==btx_fix_mcp.subservers.review.quality:[235:243]
        results = []

        for file_path in files:
            if not Path(file_path).exists():
                continue

            try:
                # Run radon cc (cyclomatic complexity) (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[41:46]
==btx_fix_mcp.subservers.review.quality:[237:244]
        for file_path in files:
            if not Path(file_path).exists():
                continue

            try:
                # Run radon cc (cyclomatic complexity)
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[75:80]
==btx_fix_mcp.subservers.review.quality:[275:282]
        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.quality.complexity:[68:80]
==btx_fix_mcp.subservers.review.quality.metrics:[79:91]
        return results

    def _analyze_raw_metrics(self, files: list[str]) -> list[dict[str, Any]]:
        """Analyze raw metrics (LOC, SLOC, comments) using radon."""
        results = []
        radon = str(get_tool_path("radon"))

        for file_path in files:
            if not Path(file_path).exists():
                continue
            try:
                result = subprocess.run( (duplicate-code)
src/btx_fix_mcp/tools_venv.py:1:0: R0801: Similar lines in 2 files
==btx_fix_mcp.subservers.review.report:[57:67]
==btx_fix_mcp.subservers.review.scope:[80:96]
        if mcp_mode:
            # MCP mode: stderr only (MCP protocol uses stdout)
            self.logger = get_mcp_logger(f"btx_fix_mcp.{name}")
        else:
            # Standalone mode: stdout only (no file logging)
            self.logger = setup_logger(name, log_file=None, level=20)  # INFO

    def validate_inputs(self) -> tuple[bool, list[str]]:
        """Validate inputs for scope analysis.

        Returns:
            Tuple of (valid, missing_items)
        """
        missing = []

        # Check repository exists (duplicate-code)

------------------------------------------------------------------
Your code has been rated at 9.89/10 (previous run: 9.89/10, +0.00)

