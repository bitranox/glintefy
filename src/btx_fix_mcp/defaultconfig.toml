# BTX Fix MCP - Default Configuration
# ====================================
# This file provides default configuration values for all tools and sub-servers.
# It is read by lib_layered_config and can be overridden by:
#   1. User config file (~/.config/btx-fix-mcp/config.toml)
#   2. Project config file (.btx-review.yaml or .btx-fix.yaml)
#   3. Environment variables (BTX_FIX_MCP_*)
#   4. Command-line arguments
#
# Configuration Priority (lowest to highest):
#   defaults < user config < project config < env vars < CLI args


# =============================================================================
# GENERAL SETTINGS
# =============================================================================

[general]
# Log level: DEBUG (10), INFO (20), WARNING (30), ERROR (40), CRITICAL (50)
log_level = "INFO"

# Enable verbose output
verbose = false

# Maximum parallel sub-server processes
max_workers = 4


# -----------------------------------------------------------------------------
# Timeout Configuration (in seconds)
# -----------------------------------------------------------------------------
[general.timeouts]
# Git operations
git_quick_op = 5          # Quick git commands (is_git_repo, get_branch, etc.)
git_status = 10           # Git status and diff operations
git_commit = 30           # Git commit (may run pre-commit hooks)
git_log = 10              # Git log and history
git_blame = 5             # Git blame operations

# Analysis tools - quick operations (< 1 minute)
tool_quick = 30           # Fast tools (radon complexity, etc.)

# Analysis tools - standard operations (1-2 minutes)
tool_analysis = 60        # Standard analysis (ruff, interrogate, bandit)

# Analysis tools - long operations (2-5 minutes)
tool_long = 120           # Slow analysis (mypy, pylint, pytest)

# Special operations
profile_tests = 300       # Test profiling with pytest (5 minutes)
vuln_scan = 120          # Dependency vulnerability scanning


# =============================================================================
# OUTPUT SETTINGS
# =============================================================================

[output]
# Output format: "markdown", "json", "html"
format = "markdown"

# Colorized terminal output
color = true

# Progress indicators
show_progress = true

# Summary style: "brief", "detailed", "verbose"
summary_style = "detailed"

# JSON output indentation (spaces)
json_indent = 2

# Chunk size for splitting large issue files
chunk_size = 50


# -----------------------------------------------------------------------------
# Display Limits for Reports
# -----------------------------------------------------------------------------
[output.display]
# Controls how many items appear in markdown summary reports
# Set to 0 for unlimited (show all items)
# Actual limit only applies if there are more items than the limit

# Scope sub-server
max_sample_files = 10     # Sample files shown in scope report

# Security sub-server
max_high_security = 10    # High severity security issues
max_medium_security = 10  # Medium severity security issues

# Deps sub-server
max_vulnerabilities = 10  # Vulnerability issues
max_outdated_packages = 10  # Outdated dependencies
max_license_issues = 5    # License violations

# Docs sub-server
max_missing_docstrings = 15  # Missing docstring examples

# Performance sub-server
max_hotspots = 10         # Performance hotspots
max_pattern_issues = 10   # Performance anti-patterns

# Final report
max_critical_display = 20  # Critical issues in final summary
max_metrics_display = 5   # Metrics shown per sub-server


# =============================================================================
# REVIEW ANALYSIS SETTINGS
# =============================================================================

[review]
# Base output directory for review analysis
output_dir = "LLM-CONTEXT/btx_fix_mcp/review"

# Sub-servers to run (in order)
# Available: scope, quality, security, deps, docs, perf, cache
subservers = ["scope", "quality", "security", "deps", "docs", "perf"]

# Stop on first failure
stop_on_failure = false


# -----------------------------------------------------------------------------
# Scope Sub-Server
# -----------------------------------------------------------------------------
[review.scope]
# Scope detection mode: "git" (uncommitted changes) or "all" (full repo)
mode = "git"

# Patterns to exclude from analysis
#
# Pattern Syntax (uses pathlib.match()):
#   *        - Matches one path component (e.g., "*.py" matches "file.py")
#   **       - Matches zero or more path components (e.g., "**/test/*" matches at any depth)
#   ?        - Matches one character
#
exclude_patterns = [
    # Dependency directories
    "**/vendor/*",
    "**/node_modules/*",
    "**/.venv/*",
    "**/__pycache__/*",

    # Build artifacts
    "**/dist/*",
    "**/build/*",

    # Version control
    "**/.git/*",

    # Compiled/generated files
    "*.pyc",
    "*.pyo",
    "*.lock",
    "*.min.js",
    "*.min.css",

    # Project-specific
    "**/LLM-CONTEXT/*",
    "**/scripts/*",
]

# Patterns to include (default: all files)
include_patterns = ["**/*"]


# -----------------------------------------------------------------------------
# Quality Sub-Server
# -----------------------------------------------------------------------------
[review.quality]
# Maximum cyclomatic complexity before flagging as issue
# A = 1-5 (low risk), B = 6-10 (moderate), C = 11-20 (complex), D = 21+ (high risk)
complexity_threshold = 10
# Complexity above this is "error" severity, below is "warning"
complexity_error_threshold = 20

# Minimum maintainability index before flagging as issue
# A = 20+ (good), B = 10-19 (moderate), C = 0-9 (poor)
maintainability_threshold = 20
# Maintainability below this is "error" severity, above is "warning"
maintainability_error_threshold = 10

# Maximum cognitive complexity threshold
cognitive_complexity_threshold = 15

# Maximum function length in lines before flagging
max_function_length = 50

# Maximum nesting depth before flagging
max_nesting_depth = 3

# --- Feature Flags ---

# Enable code duplication detection (via pylint)
enable_duplication_detection = true
# Minimum duplicate lines to report (>5 lines per old commands spec)
min_duplicate_lines = 5

# Enable static analysis (Ruff)
enable_static_analysis = true

# Enable test suite analysis
enable_test_analysis = true
# Count assertions in tests
count_test_assertions = true

# Enable architecture analysis (god objects, coupling)
enable_architecture_analysis = true
# Enable god object detection (classes with too many methods/lines)
detect_god_objects = true
# Maximum methods per class before flagging as god object
god_object_methods_threshold = 20
# Maximum lines per class before flagging as god object
god_object_lines_threshold = 500
# Enable coupling analysis
detect_high_coupling = true
# Maximum imports per module before flagging high coupling
coupling_threshold = 15

# Enable import cycle detection
enable_import_cycle_detection = true

# Enable runtime check optimization detection
enable_runtime_check_detection = true

# Enable type coverage analysis (mypy)
enable_type_coverage = true
# Minimum type coverage percentage
min_type_coverage = 80

# Enable dead code detection (vulture)
enable_dead_code_detection = true
# Minimum confidence for dead code detection (0-100)
dead_code_confidence = 80

# Enable docstring coverage analysis (interrogate)
enable_docstring_coverage = true
# Minimum docstring coverage percentage
min_docstring_coverage = 80

# Enable Halstead metrics (radon hal)
enable_halstead_metrics = true

# Enable raw metrics (LOC/SLOC/comments via radon raw)
enable_raw_metrics = true

# Enable cognitive complexity analysis
enable_cognitive_complexity = true

# Enable JavaScript/TypeScript analysis (eslint)
enable_js_analysis = true

# Enable code churn analysis (git history)
enable_code_churn = true
# Number of days to look back for churn analysis
churn_period_days = 90
# High churn threshold (commits in churn_period_days)
churn_threshold = 20

# Enable beartype runtime type checking
enable_beartype = true


# -----------------------------------------------------------------------------
# Radon Integration (for quality analysis)
# not used at the moment - radon controlled via complexity_threshold above
# -----------------------------------------------------------------------------
[review.quality.radon]
# Enable radon complexity analysis
enabled = true

# Minimum rank to show: A, B, C, D, E, F
show_complexity = "A"

# Include closures in complexity calculation
include_closures = true

# Output format: "json", "text"
output_format = "json"


# -----------------------------------------------------------------------------
# Security Sub-Server
# -----------------------------------------------------------------------------
[review.security]
# Minimum severity to report: "low", "medium", "high"
severity_threshold = "low"

# Minimum confidence to report: "low", "medium", "high"
confidence_threshold = "low"

# Bandit configuration file path (optional)
bandit_config = ""

# Skip specific Bandit test IDs (e.g., ["B101", "B102"])
skip_tests = []

# Additional paths to exclude from security scan
exclude_paths = []

# Mindset thresholds for status determination
# Number of high severity issues to trigger CRITICAL status
critical_threshold = 1

# Number of medium severity issues to trigger WARNING status
warning_threshold = 5


# -----------------------------------------------------------------------------
# Dependencies Sub-Server
# -----------------------------------------------------------------------------
[review.deps]
# Enable vulnerability scanning
scan_vulnerabilities = true

# Enable license compliance checking
check_licenses = true

# Allowed licenses (SPDX identifiers)
allowed_licenses = [
    "MIT",
    "Apache-2.0",
    "BSD-2-Clause",
    "BSD-3-Clause",
    "ISC",
    "MPL-2.0",
    "LGPL-2.1",
    "LGPL-3.0",
]

# Disallowed licenses (SPDX identifiers)
disallowed_licenses = ["GPL-3.0", "AGPL-3.0"]

# Check for outdated packages
check_outdated = true

# Maximum allowed age for dependencies (days, 0 = no limit)
max_age_days = 365


# -----------------------------------------------------------------------------
# Documentation Sub-Server
# -----------------------------------------------------------------------------
[review.docs]
# Check for missing docstrings
check_docstrings = true

# Docstring style: "google", "numpy", "sphinx"
docstring_style = "google"

# Minimum docstring coverage percentage
min_coverage = 80

# Check README exists
require_readme = true

# Check CHANGELOG exists
require_changelog = false

# Required sections in README
required_readme_sections = ["Installation", "Usage"]


# -----------------------------------------------------------------------------
# Performance Sub-Server
# -----------------------------------------------------------------------------
[review.perf]
# Enable runtime performance estimation
estimate_runtime = true

# Enable memory usage estimation
estimate_memory = true

# Flag functions with estimated high runtime (milliseconds)
runtime_threshold_ms = 100

# Flag modules with estimated high memory (MB)
memory_threshold_mb = 50

# Enable algorithm complexity detection (O(n^2), O(n!), etc.)
detect_complexity = true

# Flag nested loops deeper than threshold
nested_loop_threshold = 2


# -----------------------------------------------------------------------------
# Cache Analysis Sub-Server
# -----------------------------------------------------------------------------
[review.cache]
# LRU cache size for testing
cache_size = 128

# Minimum cache hit rate % (batch screening threshold)
hit_rate_threshold = 20.0

# Hit rate below this recommends cache removal
remove_threshold = 10.0

# Cache usage below this % of maxsize triggers size adjustment recommendation
size_adjustment_threshold = 50.0

# Minimum suggested maxsize when reducing cache size
min_suggested_maxsize = 16

# Minimum speedup % (individual validation threshold)
speedup_threshold = 5.0

# Minimum function call count to be considered a hotspot
min_calls = 100

# Minimum cumulative time (seconds) to be considered a hotspot
min_cumtime = 0.1

# Test suite timeout in seconds (applies to both batch and individual testing)
test_timeout = 300

# Number of test runs to average for individual validation (for measurement stability)
num_runs = 3

# Maximum age of profiling data in hours before warning
# Profile data older than this triggers a warning that the data may be stale
# The cache subserver also validates that profiled functions still exist in the code
max_profile_age_hours = 24.0

# Priority thresholds for hotspot analysis
# HIGH priority: call_count >= high_priority_calls AND cumtime >= high_priority_time AND indicators >= high_priority_indicators
high_priority_calls = 500
high_priority_time = 1.0
high_priority_indicators = 2

# MEDIUM priority: call_count >= medium_priority_calls OR cumtime >= medium_priority_time
medium_priority_calls = 200
medium_priority_time = 0.5


# =============================================================================
# REVIEWER MINDSETS
# =============================================================================
# Mindsets define the persona and approach for each reviewer type.
# These are used in tool descriptions, reports, and result evaluation.

[review.mindsets.quality]
role = "meticulous quality reviewer"
traits = ["pedantic", "precise", "relentlessly thorough"]

[review.mindsets.quality.approach]
every_function = "Check length (<50 lines), complexity (<10), coherence"
verify_claims = "Measure actual complexity with tools, don't assume"
no_trust = "Run analysis tools, don't guess"
standards = "No functions >50 lines, no complexity >10, no exceptions"
coherence = "Every function should be clear and inevitable"
performance = "Detect expensive runtime checks that could be cached"

[review.mindsets.quality.questions]
items = [
    "Is this function actually >50 lines? Let me measure.",
    "Is this complexity really >10? Let me run radon.",
    "Is this code duplicated elsewhere? Let me search.",
    "Could this runtime check be done once at module load?",
    "Is this nesting too deep? Let me count levels.",
]

# Judgment thresholds and verdicts
[review.mindsets.quality.judgment]
# Verdict thresholds (percentage of issues vs files)
critical_threshold = 10   # >10% critical issues = REJECT
warning_threshold = 25    # >25% warnings = NEEDS_WORK
# Verdicts
verdict_pass = "âœ… APPROVED - Code meets quality standards"
verdict_warning = "âš ï¸ APPROVED WITH COMMENTS - Minor issues found"
verdict_needs_work = "ðŸ”§ NEEDS WORK - Significant issues require attention"
verdict_reject = "âŒ REJECTED - Critical issues must be fixed"


[review.mindsets.security]
role = "paranoid security auditor"
traits = ["suspicious", "thorough", "zero-trust"]

[review.mindsets.security.approach]
every_input = "Treat all input as malicious until validated"
verify_claims = "Test security assumptions with actual attack vectors"
no_trust = "Assume breaches happen, verify defense in depth"
standards = "No hardcoded secrets, no SQL injection, no command injection"
defense = "Multiple layers of security, not single points of failure"

[review.mindsets.security.questions]
items = [
    "Can this input be manipulated? Let me test injection.",
    "Is this secret hardcoded? Let me scan for patterns.",
    "What happens if auth fails? Let me trace the flow.",
    "Is this data sanitized? Let me check validation.",
]

[review.mindsets.security.judgment]
critical_threshold = 1    # Any HIGH severity = REJECT
warning_threshold = 5     # >5 MEDIUM = NEEDS_WORK
verdict_pass = "âœ… SECURE - No significant vulnerabilities detected"
verdict_warning = "âš ï¸ ACCEPTABLE - Low-risk issues found"
verdict_needs_work = "ðŸ”§ VULNERABLE - Security issues require attention"
verdict_reject = "âŒ INSECURE - Critical vulnerabilities must be fixed immediately"


[review.mindsets.docs]
role = "meticulous documentation reviewer"
traits = ["pedantic", "completeness-focused", "clarity-obsessed"]

[review.mindsets.docs.approach]
every_api = "Check every public function, class, method for docs"
verify_claims = "Documentation must match actual code behavior"
no_trust = "Re-check every docstring against implementation"
standards = "WHY-WHAT-HOW structure, all parameters documented"
completeness = "Every parameter, return value, exception documented"

[review.mindsets.docs.questions]
items = [
    "Is this docstring describing actual behavior or just a TODO?",
    "Does this documentation match what the code actually does?",
    "Are all parameters and return values documented?",
    "Is the WHY clear, not just the WHAT?",
]

[review.mindsets.docs.judgment]
critical_threshold = 20   # >20% undocumented public APIs = REJECT
warning_threshold = 40    # >40% incomplete docs = NEEDS_WORK
verdict_pass = "âœ… WELL DOCUMENTED - Documentation is complete and accurate"
verdict_warning = "âš ï¸ ACCEPTABLE - Minor documentation gaps"
verdict_needs_work = "ðŸ”§ INCOMPLETE - Significant documentation needed"
verdict_reject = "âŒ UNDOCUMENTED - Critical documentation missing"


[review.mindsets.perf]
role = "meticulous performance reviewer"
traits = ["measurement-driven", "skeptical", "evidence-based"]

[review.mindsets.perf.approach]
every_claim = "Verify every performance assertion with profiling"
verify_claims = "Profile with actual test suite, never synthetic benchmarks"
no_trust = "Don't believe claims without benchmarks"
standards = "Improvements must be >5% to justify complexity"
evidence = "Show profiling data, cache hit rates, before/after metrics"

[review.mindsets.perf.questions]
items = [
    "Is this actually faster? Let me profile it.",
    "What's the cache hit rate with REAL data? Let me measure.",
    "Is this optimization worth the complexity? Show me the numbers.",
    "Where's the actual bottleneck? Let me profile with the test suite.",
]

[review.mindsets.perf.judgment]
critical_threshold = 5    # >5% severe bottlenecks = REJECT
warning_threshold = 15    # >15% potential issues = NEEDS_WORK
verdict_pass = "âœ… PERFORMANT - No significant performance issues"
verdict_warning = "âš ï¸ ACCEPTABLE - Minor optimization opportunities"
verdict_needs_work = "ðŸ”§ SLOW - Performance issues require attention"
verdict_reject = "âŒ UNACCEPTABLE - Critical performance problems"


[review.mindsets.deps]
role = "paranoid dependency auditor"
traits = ["vigilant", "risk-averse", "thorough"]

[review.mindsets.deps.approach]
security_first = "Scan all dependencies for known vulnerabilities (CVEs)"
freshness = "Check for outdated packages that may have security patches"
license_compliance = "Verify all licenses are compatible with project"
supply_chain = "Assess dependency tree depth and trusted sources"

[review.mindsets.deps.questions]
items = [
    "Are there any known CVEs in these dependencies? Let me check pip-audit.",
    "How outdated are these packages? What security patches are we missing?",
    "Are all licenses compatible? Any GPL contamination risks?",
    "How deep is the dependency tree? Are we pulling in untrusted sources?",
]

[review.mindsets.deps.judgment]
critical_threshold = 1    # >0 critical vulnerabilities = REJECT
warning_threshold = 20    # >20% outdated = NEEDS_WORK
verdict_pass = "âœ… SECURE - Dependencies are secure and up-to-date"
verdict_warning = "âš ï¸ UPDATE RECOMMENDED - Minor updates available"
verdict_needs_work = "ðŸ”§ ACTION NEEDED - Security patches required"
verdict_reject = "âŒ VULNERABLE - Critical security issues found"


[review.mindsets.cache]
role = "cache optimization reviewer"
traits = ["skeptical", "evidence-driven", "data-obsessed"]

[review.mindsets.cache.approach]
real_test_data = "Profile with REAL test suite, never synthetic benchmarks"
evidence_required = "Show cache hit rate, speedup metrics for every recommendation"
no_trust = "Don't believe claims without profiling data"
minimum_threshold = "Hit rate >20%, speedup >5% to justify complexity"

[review.mindsets.cache.questions]
items = [
    "Is this actually faster? Let me profile it.",
    "What's the cache hit rate with REAL data? Let me measure.",
    "Is this optimization worth the complexity? Show me the numbers.",
    "Where's the actual bottleneck? Let me profile with the test suite.",
]

[review.mindsets.cache.judgment]
critical_threshold = 0    # No critical issues concept for cache analysis
warning_threshold = 0     # No warnings concept for cache analysis
verdict_pass = "âœ… RECOMMENDATIONS AVAILABLE - Evidence-based caching opportunities found"
verdict_warning = "âš ï¸ LIMITED OPPORTUNITIES - Some candidates found but marginal benefits"
verdict_needs_work = "ðŸ”§ NO RECOMMENDATIONS - No functions met both hit rate and speedup thresholds"
verdict_reject = "âŒ NO ANALYSIS - Missing profiling data or pure functions"


# =============================================================================
# INTERNAL LLM CONFIGURATION
# =============================================================================
# The MCP server can use an internal LLM client for classification, analysis,
# and verification tasks. This is SEPARATE from the caller's context and does
# not affect the user's token budget or conversation history.

[llm]
# Enable internal LLM usage for analysis enhancement
# When disabled, the server falls back to rule-based analysis
enable_internal_llm = false

# LLM provider: "anthropic", "openai", "ollama", "openai_compatible"
provider = "anthropic"

# Model for complex analysis (reasoning, suggestions, verification)
model = "claude-3-5-sonnet-20241022"

# Fast model for simple classification tasks
fast_model = "claude-3-5-haiku-20241022"

# Maximum tokens for different operation types
max_tokens_classification = 10   # Simple severity/category classification
max_tokens_suggestion = 500      # Fix strategy suggestions
max_tokens_verification = 300    # Fix verification

# Rate limiting (calls per minute to avoid hitting API limits)
rate_limit = 60

# Enable prompt caching to save tokens on repeated analysis
enable_prompt_caching = true

# Feature flags - enable specific LLM use cases
[llm.features]
# Classify issue severity (low/medium/high/critical) with LLM
# Falls back to rule-based if disabled or on error
classify_severity = false

# Suggest fix strategies for code issues (more expensive)
# Provides structured fix recommendations with steps
suggest_fixes = false

# Verify that fixes actually resolve reported issues (future feature)
# Cross-checks fix results against issue descriptions
verify_fixes = false

# Generate commit messages for automated fixes
# Creates conventional commit messages from change summaries
generate_commit_messages = false

# Analyze code patterns for deeper insights (experimental)
# Identifies anti-patterns and suggests improvements
analyze_patterns = false


# =============================================================================
# FIX ANALYSIS SETTINGS
# not used at the moment - fix server not yet implemented
# =============================================================================

[fix]
# Base output directory for fix operations
output_dir = "LLM-CONTEXT/btx_fix_mcp/fix"

# Enable evidence-based fixing (measure -> fix -> verify)
evidence_based = true

# Number of verification attempts before accepting fix
verification_attempts = 3

# Auto-revert on failed verification
auto_revert = true


# -----------------------------------------------------------------------------
# Fix Scope Settings
# not used at the moment - fix server not yet implemented
# -----------------------------------------------------------------------------
[fix.scope]
# Scope mode for fix operations: "single", "related", "module"
mode = "single"

# Include related test files
include_tests = true

# Maximum files to include in fix scope
max_files = 10


# -----------------------------------------------------------------------------
# Test Runner Settings
# not used at the moment - fix server not yet implemented
# -----------------------------------------------------------------------------
[fix.test]
# Test framework: "pytest", "unittest", "nose"
framework = "pytest"

# Test command (if not using framework auto-detection)
command = ""

# Test timeout in seconds
timeout = 300

# Run tests in parallel
parallel = true

# Stop on first test failure
fail_fast = false

# Coverage threshold for pass (percentage, 0 = disabled)
min_coverage = 0


# -----------------------------------------------------------------------------
# Linter Settings
# not used at the moment - fix server not yet implemented
# -----------------------------------------------------------------------------
[fix.lint]
# Linters to run: "ruff", "pylint", "flake8", "mypy"
linters = ["ruff", "mypy"]

# Auto-fix safe linting issues
auto_fix = true

# Fail on any linting error
strict = false


# =============================================================================
# TOOL-SPECIFIC SETTINGS
# not used at the moment - tools use their own config files or hardcoded defaults
# These settings are reserved for future direct tool invocation
# =============================================================================

# -----------------------------------------------------------------------------
# Bandit (Security Scanner)
# not used at the moment - bandit invoked with hardcoded args in security.py
# -----------------------------------------------------------------------------
[tools.bandit]
# Bandit severity levels: 1 (LOW), 2 (MEDIUM), 3 (HIGH)
min_severity = 1

# Bandit confidence levels: 1 (LOW), 2 (MEDIUM), 3 (HIGH)
min_confidence = 1

# Output format: "json", "txt", "html", "csv"
format = "json"

# Recursive scan
recursive = true

# Number of parallel processes
processes = 4


# -----------------------------------------------------------------------------
# Radon (Complexity Analyzer)
# Used by ComplexityAnalyzer for cyclomatic complexity analysis
# -----------------------------------------------------------------------------
[tools.radon]
# Show complexity for all ranks (A through F)
show_all = true

# Include average complexity in output
show_average = true

# Sort results by: "SCORE" (complexity), "LINES" (line number), "ALPHA" (alphabetical)
sort_by = "SCORE"


# -----------------------------------------------------------------------------
# Pylint Duplication Detection
# Only used for duplicate-code detection (R0801), not full pylint analysis
# -----------------------------------------------------------------------------
[tools.pylint]
# Ignore comments when computing similarities
ignore_comments = true

# Ignore docstrings when computing similarities
ignore_docstrings = true

# Ignore imports when computing similarities
ignore_imports = true

# Ignore function signatures when computing similarities
ignore_signatures = true


# -----------------------------------------------------------------------------
# Ruff (Fast Linter)
# Used by StaticAnalyzer for linting and style checks
# -----------------------------------------------------------------------------
[tools.ruff]
# Line length
line_length = 88

# Target Python version
target_version = "py313"

# Rules to enable (use "ALL" for all rules)
select = ["E", "F", "W", "I", "N", "UP", "B", "C4", "SIM"]

# Rules to ignore
ignore = ["E501"]

# Auto-fix
fix = true

# Unsafe fixes allowed
unsafe_fixes = false


# -----------------------------------------------------------------------------
# MyPy (Type Checker)
# Used by TypeAnalyzer for type coverage analysis
# -----------------------------------------------------------------------------
[tools.mypy]
# Python version
python_version = "3.13"

# Strict mode
strict = false

# Ignore missing imports
ignore_missing_imports = true

# Show error codes
show_error_codes = true

# Pretty output
pretty = true


# -----------------------------------------------------------------------------
# Black (Formatter)
# not used at the moment - black not invoked by any subserver
# -----------------------------------------------------------------------------
[tools.black]
# Line length
line_length = 88

# Target Python versions
target_version = ["py313"]

# String normalization
skip_string_normalization = false

# Magic trailing comma
skip_magic_trailing_comma = false


# -----------------------------------------------------------------------------
# Pytest (Test Runner)
# Used by cache validation and runtime type checking
# -----------------------------------------------------------------------------
[tools.pytest]
# Test paths
testpaths = ["tests"]

# Python files pattern
python_files = ["test_*.py", "*_test.py"]

# Python classes pattern
python_classes = ["Test*"]

# Python functions pattern
python_functions = ["test_*"]

# Minimum coverage percentage
cov_fail_under = 80

# Coverage report format
cov_report = ["term-missing", "html"]

# Markers to run
markers = []

# Plugins to disable
disabled_plugins = []


# =============================================================================
# GIT SETTINGS
# Used by GitOperations for automated commits and branch management
# =============================================================================

[git]
# Commit message prefix for automated commits
# Prepended to all commit messages created by btx-fix
commit_prefix = "[btx-fix]"

# Auto-commit fixes after successful verification
# When enabled, fixes are automatically committed without user confirmation
auto_commit = false

# Sign commits with GPG (-S flag)
# Requires GPG to be configured in git
sign_commits = false

# Create a new branch for fixes before making changes
# Uses branch_template to generate branch name
create_branch = false

# Branch name template for fix branches
# {issue_id} is replaced with the issue identifier
branch_template = "fix/{issue_id}"
